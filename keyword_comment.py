{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOWyLK4ywi9LjqGwvlpC15E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/XiaodiYang2001/Weibo-Content-Analysis/blob/master/keyword_comment.py\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSYlfCNLvxpV"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import json\n",
        "import re\n",
        "from scrapy import Spider, Request\n",
        "from spiders.common import parse_tweet_info, parse_long_tweet, parse_user_info, parse_time, url_to_mid\n",
        "\n",
        "class TweetSpiderByKeyword(Spider):\n",
        "    \"\"\"\n",
        "    关键词搜索采集\n",
        "    \"\"\"\n",
        "    name = \"tweet_spider_by_keyword\"\n",
        "    base_url = \"https://s.weibo.com/\"\n",
        "\n",
        "    def start_requests(self):\n",
        "        \"\"\"\n",
        "        爬虫入口\n",
        "        \"\"\"\n",
        "        # 这里keywords可替换成实际待采集的数据\n",
        "        keywords = ['北大首映', '坠落的审判北大', '坠落的审判北大首映']\n",
        "        # 这里的时间可替换成实际需要的时间段\n",
        "        start_time = datetime.datetime(year=2024, month=3, day=24, hour=21)\n",
        "        end_time = datetime.datetime(year=2024, month=3, day=29, hour=21)\n",
        "        # 是否按照小时进行切分，数据量更大; 对于非热门关键词**不需要**按照小时切分\n",
        "        is_split_by_hour = True\n",
        "        for keyword in keywords:\n",
        "            if not is_split_by_hour:\n",
        "                _start_time = start_time.strftime(\"%Y-%m-%d-%H\")\n",
        "                _end_time = end_time.strftime(\"%Y-%m-%d-%H\")\n",
        "                url = f\"https://s.weibo.com/weibo?q={keyword}&timescope=custom%3A{_start_time}%3A{_end_time}&page=1\"\n",
        "                yield Request(url, callback=self.parse, meta={'keyword': keyword})\n",
        "            else:\n",
        "                time_cur = start_time\n",
        "                while time_cur < end_time:\n",
        "                    _start_time = time_cur.strftime(\"%Y-%m-%d-%H\")\n",
        "                    _end_time = (time_cur + datetime.timedelta(hours=1)).strftime(\"%Y-%m-%d-%H\")\n",
        "                    url = f\"https://s.weibo.com/weibo?q={keyword}&timescope=custom%3A{_start_time}%3A{_end_time}&page=1\"\n",
        "                    yield Request(url, callback=self.parse, meta={'keyword': keyword})\n",
        "                    time_cur = time_cur + datetime.timedelta(hours=1)\n",
        "\n",
        "    def parse(self, response, **kwargs):\n",
        "        \"\"\"\n",
        "        网页解析\n",
        "        \"\"\"\n",
        "        html = response.text\n",
        "        if '<p>抱歉，未找到相关结果。</p>' in html:\n",
        "            self.logger.info(f'no search result. url: {response.url}')\n",
        "            return\n",
        "        tweets_infos = re.findall('<div class=\"from\"\\s+>(.*?)</div>', html, re.DOTALL)\n",
        "        for tweets_info in tweets_infos:\n",
        "            tweet_ids = re.findall(r'weibo\\.com/\\d+/(.+?)\\?refer_flag=1001030103_\" ', tweets_info)\n",
        "            for tweet_id in tweet_ids:\n",
        "                url = f\"https://weibo.com/ajax/statuses/show?id={tweet_id}\"\n",
        "                yield Request(url, callback=self.parse_tweet, meta=response.meta, priority=10)\n",
        "        next_page = re.search('<a href=\"(.*?)\" class=\"next\">下一页</a>', html)\n",
        "        if next_page:\n",
        "            url = \"https://s.weibo.com\" + next_page.group(1)\n",
        "            yield Request(url, callback=self.parse, meta=response.meta)\n",
        "\n",
        "    def parse_tweet(self, response):\n",
        "        \"\"\"\n",
        "        解析推文\n",
        "        \"\"\"\n",
        "        data = json.loads(response.text)\n",
        "        item = parse_tweet_info(data)\n",
        "        item['keyword'] = response.meta['keyword']\n",
        "        if item['isLongText']:\n",
        "            url = \"https://weibo.com/ajax/statuses/longtext?id=\" + item['mblogid']\n",
        "            yield Request(url, callback=parse_long_tweet, meta={'item': item}, priority=20)\n",
        "        else:\n",
        "            yield item\n",
        "\n",
        "        # 抓取评论信息\n",
        "        comments_url = f\"https://weibo.com/ajax/statuses/buildComments?is_reload=1&id={mid}&is_show_bulletin=2&is_mix=0&count=20\"\n",
        "        yield Request(comments_url, callback=self.parse_comments, meta={'item': item, 'source_url': comments_url}, priority=30)\n",
        "\n",
        "        # 抓取转发信息\n",
        "        reposts_url = f\"https://weibo.com/ajax/statuses/repostTimeline?id={mid}&page=1&moduleID=feed&count=10\"\n",
        "        yield Request(reposts_url, callback=self.parse_reposts, meta={'item': item, 'page_num': 1, 'mid': mid}, priority=30)\n",
        "\n",
        "        # 抓取点赞信息\n",
        "        attitudes_url = f\"https://weibo.com/ajax/statuses/attitudes?id={mid}&page=1&count=20\"\n",
        "        yield Request(attitudes_url, callback=self.parse_attitudes, meta={'item': item, 'page_num': 1, 'mid': mid}, priority=30)\n",
        "\n",
        "    def parse_comments(self, response):\n",
        "        \"\"\"\n",
        "        解析评论内容\n",
        "        \"\"\"\n",
        "        item = response.meta['item']\n",
        "        data = json.loads(response.text)\n",
        "        item['comments'] = [{'user_id': comment['user']['id'], 'nick_name': comment['user']['screen_name'], 'comment': comment['text_raw']} for comment in data['data']]\n",
        "        yield item\n",
        "\n",
        "        # 解析二级评论\n",
        "        for comment_info in data['data']:\n",
        "            if 'more_info' in comment_info:\n",
        "                url = f\"https://weibo.com/ajax/statuses/buildComments?is_reload=1&id={comment_info['id']}\" \\\n",
        "                      f\"&is_show_bulletin=2&is_mix=1&fetch_level=1&max_id=0&count=100\"\n",
        "                yield Request(url, callback=self.parse_comments, priority=20)\n",
        "\n",
        "        if data.get('max_id', 0) != 0 and 'fetch_level=1' not in response.url:\n",
        "            url = response.meta['source_url'] + '&max_id=' + str(data['max_id'])\n",
        "            yield Request(url, callback=self.parse_comments, meta=response.meta)\n",
        "\n",
        "    def parse_reposts(self, response):\n",
        "        \"\"\"\n",
        "        解析转发内容\n",
        "        \"\"\"\n",
        "        item = response.meta['item']\n",
        "        data = json.loads(response.text)\n",
        "        if 'reposts' not in item:\n",
        "            item['reposts'] = []\n",
        "        item['reposts'].extend([{'user_id': repost['user']['id'], 'nick_name': repost['user']['screen_name'], 'repost': repost['text_raw']} for repost in data['data']])\n",
        "        yield item\n",
        "\n",
        "    def parse_attitudes(self, response):\n",
        "        \"\"\"\n",
        "        解析点赞用户\n",
        "        \"\"\"\n",
        "        item = response.meta['item']\n",
        "        data = json.loads(response.text)\n",
        "        if 'attitudes' not in item:\n",
        "            item['attitudes'] = []\n",
        "        item['attitudes'].extend([{'user_id': attitude['user']['id'], 'nick_name': attitude['user']['screen_name']} for attitude in data['data']])\n",
        "        yield item"
      ]
    }
  ]
}